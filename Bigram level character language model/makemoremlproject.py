# -*- coding: utf-8 -*-
"""MakeMoreMLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1skJ5GavKseNwGUvkFJqVSH4tZFP_meQe
"""

words = open('names.txt' , 'r').read().splitlines()

"""**BIGRAM CHARACTER LEVEL LANGUAGE MODEL**"""

words[:10]

len(words)

min(len(w) for w in words)

max(len(w) for w in words)

b ={}
for w in words:
  chs = ['<S>'] + list(w) + ['<E>']
  for ch1 , ch2 in zip(chs , chs[1:]):
    bigram = (ch1 , ch2)
    b[bigram] = b.get(bigram , 0) +1

sorted(b.items() , key = lambda kv: -kv[1])

import torch

N = torch.zeros((27 ,27) , dtype = torch.int32)

chars = sorted(list(set(''.join(words))))
stoi = {s : i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}

for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1 , ch2 in zip(chs , chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    N[ix1,ix2] += 1

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize =(16,16))
plt.imshow(N , cmap = 'Blues')
for i in range(27):
  for j in range(27):
    chstr = itos[i] + itos[j]
    plt.text(j , i , chstr , ha ="center" , va = "bottom" , color = 'gray')
    plt.text(j , i , N[i , j].item() , ha = "center" , va = "top" , color = 'gray')
plt.axis('off');

N[0]

p = N[0].float()
p = p/ p.sum()
p

g = torch.Generator().manual_seed(2147483647)
ix = torch.multinomial(p , num_samples = 1 , replacement = True, generator = g).item()
itos[ix]

g = torch.Generator().manual_seed(2147483647)
p = torch.rand(3 , generator = g)
p = p/p.sum()
p

torch.multinomial(p, num_samples = 100, replacement = True , generator = g)

p.shape

P = (N+1).float()
P /= P.sum(1 , keepdim = True)

P.shape

g = torch.Generator().manual_seed(2147483647)
for i in range(5):
  out = []
  ix = 0
  while True:
    p = N[ix].float()
    p = p/p.sum()
    ix = torch.multinomial(p, num_samples = 1, replacement = True , generator = g).item()
    out.append(itos[ix])
    if ix==0:
      break
  print(''.join(out))

# log(a*b*c) = log(a) + log(b) + log(c)

# GOAL : maximize likelihood of the data w.r.to model parameters (statistical modeling )
# equivalent to maximizing the log likelihood (because log is monotonic)
# equivalent to minimizing the negative log likelihood
# equivalent to minimizing the average negative log likelihood

log_likelihood = 0.0
n =0
#for w in words:
for w in ["andrejq"]:
  chs = ['.'] + list(w) + ['.']
  for ch1 , ch2 in zip(chs , chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    prob = P[ix1 , ix2]
    logprob = torch.log(prob)
    log_likelihood += logprob
    n += 1
    print(f'{ch1}{ch2}: {prob: .4f} {logprob: .4f}')
print(f'{log_likelihood = }')
nll = -log_likelihood
print(f'{nll =}')
print(f'{nll/n}')

"""# **NURAL NETWORK BIGRAM LANGUAGE MODEL**"""

# create the training set of bigrams (x ,y)
xs , ys = [] , []

for w in words[:1]:
  chs = ['.'] + list(w) + ['.']
  for ch1 , ch2 in zip(chs , chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    print(ch1 , ch2)
    xs.append(ix1)
    ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print('number of examples:' , num)

xs

ys

xs.dtype

import torch.nn.functional as F
xenc = F.one_hot(xs , num_classes = 27).float()
xenc

xenc.shape

plt.imshow(xenc)

xenc.dtype

W = torch.randn((27 ,27))
xenc @ W

# because xenc is 5 , 27
# and W is 27 ,27
# hence output is = 5 ,27

(xenc @ W).shape

logits = xenc @ W # log-counts
counts = logits.exp() #equivalent N
probs = counts / counts.sum(1 , keepdims = True)
probs

probs.shape

probs[0].sum()

# SUMMARY--------------------------->>>>>>>>>>>>>.

xs

ys

# randomly initialize 27 neurons weights. each neuron receives 27 inputs
g = torch.Generator().manual_seed(2147483647)
W =torch.randn((27 ,27) , generator = g)

xenc  = F.one_hot(xs , num_classes = 27).float() # input to the network : one-hot encoding
logits = xenc @ W # predict log counts
counts = logits.exp() # counts , equivalent to N
probs = counts / counts.sum(1 , keepdims = True) # prob for next character

# btw the last two lines together call SOFTMAX

probs.shape

nlls = torch.zeros(5)
for i in range(5):
  # i-th bigram:
  x = xs[i].item() #input character index
  y= ys[i].item() # label character index
  print('---------------')
  print(f'bigram example {i +1}: {itos[x]}{itos[y]} (indexs {x} , {y})')
  print('intput to the nural network:' , x)
  print('output probabilities from the nural net:' , probs[i])
  print('label {actual next character} : ' , y)
  p = probs[i , y]
  print('probability assigned by the net to the correct character:' , p.item())
  logp = torch.log(p)
  print('log likelihood:' , logp.item())
  nll = -logp
  print('negative log likelihood:' , nll.item())
  nlls[i] = nll

print('===========')
print('average negative log likelihoood, i.e loss =  ' , nlls.mean().item())

# --------------!!!!!!!!optimization !!!----------------

xs

ys

# randomly initialize 27 neurons weights. each neuron receives 27 inputs
g = torch.Generator().manual_seed(2147483647)
W =torch.randn((27 ,27) , generator = g , requires_grad = True )

xenc  = F.one_hot(xs , num_classes = 27).float() # input to the network : one-hot encoding
logits = xenc @ W # predict log counts
counts = logits.exp() # counts , equivalent to N
probs = counts / counts.sum(1 , keepdims = True) # prob for next character
loss = -probs[torch.arange(5) , ys].log().mean()

print(loss.item())

# backward pass
W.grad = None # set to zero the gradient
loss.backward()

# update
W.data += -0.1 + W.grad

# create the training set of bigrams (x ,y)
xs , ys = [] , []

for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1 , ch2 in zip(chs , chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    xs.append(ix1)
    ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print('number of examples:' , num)
# randomly initialize 27 neurons weights. each neuron receives 27 inputs
g = torch.Generator().manual_seed(2147483647)
W =torch.randn((27 ,27) , generator = g , requires_grad = True )

for k in range(100):

  xenc  = F.one_hot(xs , num_classes = 27).float() # input to the network : one-hot encoding
  logits = xenc @ W # predict log counts
  counts = logits.exp() # counts , equivalent to N
  probs = counts / counts.sum(1 , keepdims = True) # prob for next character
  loss = -probs[torch.arange(num) , ys].log().mean()
  print(loss.item())
  # backward pass
  W.grad = None # set to zero the gradient
  loss.backward()

  # update
  W.data -= -0.1 + W.grad

# finally , sample from the 'neural net' model

g = torch.Generator().manual_seed(2147483647)

for i in range(5):
  out = []
  ix =0
  while True:
    #before
   # p = P[ix]
    # now

    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()
    logits = xenc @ W # predict log counts
    counts = logits.exp()  #count , equivalent to N
    p = counts / counts.sum(1 , keepdims = True) # prob for next character

    ix = torch.multinomial(p , num_samples = 1 ,replacement = True , generator = g).item()
    out.append(itos[ix])
    if ix == 0 :
      break
  print(''.join(out))

